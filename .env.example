# LLM Evaluator Configuration
# Copy this file to .env and customize as needed

# Environment
LLM_EVAL_ENVIRONMENT=dev  # dev, test, prod

# Model Settings
LLM_EVAL_DEFAULT_MODEL=llama3.2:1b
LLM_EVAL_DEFAULT_PROVIDER=ollama  # ollama, openai, anthropic

# Generation Settings
LLM_EVAL_DEFAULT_TEMPERATURE=0.7
LLM_EVAL_DEFAULT_MAX_TOKENS=500
LLM_EVAL_DEFAULT_TIMEOUT=30
LLM_EVAL_DEFAULT_RETRY_ATTEMPTS=3

# Evaluation Settings
LLM_EVAL_PERFORMANCE_SAMPLES=10

# Logging
LLM_EVAL_LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
LLM_EVAL_LOG_FILE=  # Leave empty for console only

# Output
LLM_EVAL_OUTPUT_DIR=outputs

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_PULL_MISSING_MODELS=false
OLLAMA_KEEP_ALIVE=5m

# Benchmark Settings
BENCHMARK_USE_DEMO_BENCHMARKS=true
BENCHMARK_MMLU_SUBSET=  # e.g., abstract_algebra
BENCHMARK_CACHE_DIR=~/.cache/llm_evaluator
