# LLM Evaluation Suite

> Comprehensive evaluation framework for testing Large Language Model capabilities across multiple dimensions

## ğŸ¯ Project Overview

Week 3-4 of AI Safety Engineer roadmap. Building on Week 1-2 security testing, this project focuses on ML evaluation metrics, performance benchmarking, and statistical analysis of LLM outputs.

## ğŸš€ Features

- **Performance Metrics**: Response time, token efficiency, memory usage
- **Quality Metrics**: Factual accuracy, coherence, hallucination detection
- **Standard Benchmarks**: MMLU, TruthfulQA, HellaSwag integration
- **Comparison Dashboard**: Side-by-side model analysis with visualizations
- **Statistical Analysis**: Significance testing, confidence intervals

## ğŸ“¦ Installation

```bash
# Clone repository
git clone https://github.com/NahuelGiudizi/llm-evaluation.git
cd llm-evaluation

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e .
```

## ğŸ”§ Quick Start

```python
from llm_evaluator import ModelEvaluator

# Initialize evaluator
evaluator = ModelEvaluator(model="llama3.2:1b")

# Run comprehensive evaluation
results = evaluator.evaluate_all()

# Print summary
print(f"Accuracy: {results.accuracy:.2%}")
print(f"Avg Response Time: {results.avg_response_time:.2f}s")
print(f"Hallucination Rate: {results.hallucination_rate:.2%}")

# Generate report
evaluator.generate_report(output="evaluation_report.html")
```

## ğŸ“Š Supported Benchmarks

- **MMLU** (Massive Multitask Language Understanding) - 57 subjects
- **TruthfulQA** - Truthfulness and informativeness
- **HellaSwag** - Commonsense reasoning
- **Custom Datasets** - Domain-specific evaluations

## ğŸ“ Learning Outcomes

- âœ… ML evaluation metrics (accuracy, precision, recall, F1)
- âœ… Statistical significance testing
- âœ… Model performance profiling
- âœ… Data visualization with matplotlib/plotly
- âœ… Integration with HuggingFace ecosystem

## ğŸ“ Project Structure

```
llm-evaluation/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ llm_evaluator/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ evaluator.py       # Main evaluation class
â”‚       â”œâ”€â”€ metrics.py          # Performance & quality metrics
â”‚       â”œâ”€â”€ benchmarks.py       # Standard benchmark integrations
â”‚       â””â”€â”€ visualizer.py       # Dashboard generation
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_evaluator.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb          # Interactive analysis
â”œâ”€â”€ data/
â”‚   â””â”€â”€ results/                # Evaluation results storage
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ EXAMPLES.md
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py
```

## ğŸ› ï¸ Tech Stack

- **Python 3.11+**
- **Ollama** - Local LLM runtime
- **HuggingFace Datasets** - Standard benchmarks
- **lm-evaluation-harness** - EleutherAI evaluation framework
- **scikit-learn** - Statistical metrics
- **matplotlib/plotly** - Visualizations
- **Jupyter** - Interactive analysis

## ğŸ”— Integration with Week 1-2

Combine security testing with performance evaluation:

```python
from ai_safety_tester import SimpleAITester, SeverityScorer
from llm_evaluator import ModelEvaluator

# Initialize both
security_tester = SimpleAITester(model="llama3.2:1b")
performance_evaluator = ModelEvaluator(model="llama3.2:1b")

# Run combined analysis
security_score = security_tester.run_all_tests()
performance_metrics = performance_evaluator.evaluate_all()

# Holistic assessment
print(f"Security: {security_score.aggregate_score}/10")
print(f"Performance: {performance_metrics.overall_score:.2f}")
```

## ğŸ“ˆ Expected Results

- Benchmark 4+ models on standard datasets
- Statistical comparison with confidence intervals
- Identify strengths/weaknesses per model
- Reusable evaluation pipeline

## ğŸ“ Notes

- **Cost**: $0 (100% local with Ollama)
- **Models Tested**: Llama 3.2, Mistral, Phi-3, Gemma
- **Evaluation Time**: ~10 minutes per model
- **Output**: HTML/JSON/Markdown reports

## ğŸ”— Resources

- [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [HuggingFace Datasets](https://huggingface.co/docs/datasets/)
- [MMLU Benchmark](https://github.com/hendrycks/test)

---

**Author**: Nahuel  
**Date**: November 2025  
**Project**: AI Safety & Alignment Testing Roadmap - Week 3-4
